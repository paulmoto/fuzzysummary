{
    "contents" : "require(tm)\nrequire(frbs)\nrequire(SnowballC)\nrequire(NLP)\nrequire(stringr)\nrequire(openNLP)\nrequire(openNLPmodels.en)\nrequire(lsa)\nrequire(igraph)\n\ntext<-gsub(\"\\n\",\"\\n\\n\",text)\nparanot <-\n  Annotator(function(s, a = Annotation()) {\n    spans <- blankline_tokenizer(s)\n    n <- length(spans)\n    ## Need n consecutive ids, starting with the next \"free\"\n    ## one:\n    from <- next_id(a$id)\n    Annotation(seq(from = from, length.out = n),\n               rep.int(\"paragraph\", n),\n               spans$start,\n               spans$end)\n  },\n  \"A paragraph token annotator based on blankline_tokenizer().\")\n\n\n\n#text<-gsub(\"\\n\",\" \",text)\n#corpus<-VCorpus(VectorSource(text))\nanot<-Maxent_Sent_Token_Annotator(language = \"en\", probs = FALSE, model = NULL)\nwanot <- Maxent_Word_Token_Annotator()\nposanot <- Maxent_POS_Tag_Annotator()\nann <- annotate(text,\n               list(anot,\n                    wanot,\n                    posanot,paranot))\n\nanntext<-AnnotatedPlainTextDocument(text,ann)\nsent_ann<-annotate(text,anot)\n\nparalength<-vector()\nfor(i in 1:length(paras(anntext))){\n  paralength[i]<-length(paras(anntext)[[i]])  \n}\n\nparapos<-vector()\nfor(i in 1:length(paralength)){\n  for(j in 1:paralength[i]){\n    parapos<-c(parapos,paralength[i]-(j-1))\n  }\n}\n\n\n\n\norigsent<-vector()\nfor(i in 1:length(sent_ann)){\n  origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])\n}\n\ncorpus<-VCorpus(VectorSource(origsent))\ncorpus<-tm_map(corpus, stripWhitespace)\ncorpus<-tm_map(corpus,content_transformer(tolower))\ncorpus<-tm_map(corpus,removeWords,stopwords(\"english\"))\ncorpus<-tm_map(corpus,stemDocument)\n\nsent_words<-list()\nfor(i in 1:length(corpus)){\nsent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==\"\"]\n}\n\ndict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())\n\n#Title words Feature\ntitle<-\"New Orleans Saints' Will Smith plans to be back as long as the Saints want him\"\ntwords<-stripWhitespace(title)\ntwords<-tolower(unlist(strsplit(title,\" \")))\ntwords<-twords[!twords %in% stopwords('english')]\ntwords<-stemDocument(twords)\n\n\n#wordcount counts number of times x appears in y\nwordcount<- function(x,y){\n  return(length(grep(sprintf(\"^%s$\",x),y)))\n  }\n\n\n#frequency of words in document\nwordfreq<-vector()\nwordoccur<-vector() #sentences containing each word\nwordscore<-vector()\nfor(i in 1:nrow(dict)){\n  wordfreq[i]<-sum(dict[i,])\n  wordoccur[i]<-sum(dict[i,]>0)\n  wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])\n}\nwordfreq<-data.frame(dict[[6]]$Terms,wordfreq)\nthematic<-wordfreq[order(-wordfreq[,2])[1:5],1]\n\n\n\n\n\n#sim<-matrix(NA,length(corpus),length(corpus))\n#maxes<-vector()\n#token matching\n\n#for(i in 1:nrow(sim)){\n#  for(j in 1:ncol(sim)){\n#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))                        \n#  }}\n#diag(sim)<-0\n#for(i in 1:nrow(sim)){\n#  if(max(sim[,i])>0){\n#  sim[,i]<-sim[,i]/max(sim[,i])\n#  }\n#}\nsentencevectors<-matrix(NA,nrow(dict),length(origsent))\nfor(i in 1:ncol(sentencevectors)){\n  sentencevectors[,i]<-as.vector(dict[,i]*wordscore)\n}\n\nsim<-matrix(NA,length(origsent),length(origsent))\nfor(i in 1:nrow(sim)){\n  for(j in 1:ncol(sim)){\n    if(!i==j){\n     sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))\n    }\n  }\n}\ndiag(sim)<-0\n\n\n\n\nfeatures<-matrix(0,length(corpus),8)\nfor(i in 1:nrow(features)){\n  if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words\n  features[i,2]<- length(sent_words[[i]]) #sentence length\n  features[i,3]<- NA#sentence position\n  if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]],\" \")[[1]])))/length(sent_words[[i]])}#numerical data\n  features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words\n  features[i,6]<- sum(sim[,i]) #similarity\n  features[i,7]<-sum(dict[,i]*wordscore) #word frequency\n  if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],\"[A-Z][a-z]\")/length(sent_words[[i]])} #Proper nouns?\n}\n\n#normalizing features\nfeatures[1:length(parapos),3]<-parapos\nfeatures[length(parapos):nrow(features),3]<-0\nfeatures[,3]<-features[,3]/max(features[,3])\nfeatures[,2]<-features[,2]/max(features[,2])\nfeatures[,5]<-features[,5]/max(features[,5])\nfeatures[,6]<-features[,6]/max(features[,6])\nfeatures[,7]<-features[,7]/max(features[,7])\n\nfeatureweights<-c(1,1,1,1,1,1,1,1)\nsent_scores<-vector()\nfor(i in 1:nrow(features)){\n  sent_scores[i]<-sum(features[i,]*featureweights)\n}\nsummary<-data.frame(origsent,sent_scores)\nsummary<-summary[order(-sent_scores),]\n\nsumstat<-function(n){\n  return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))\n}\n\n\n#SVD<-lsa(dict)$dk\n#\n#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))\n#cosine distance = angle between vectors [0,pi/2]\n#for(i in 1:nrow(cosadj)){\n#  for(j in 1:ncol(cosadj)){\n#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}\n#  }\n#}\n#diag(cosadj)<-0\n\n#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')\n#pgr<-page.rank(g)$vector\n#summ<-data.frame(origsent,pgr)[order(-pgr),]\n#cosine distance = cosine(inspect(dict))\n",
    "created" : 1426088098151.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4182391010",
    "id" : "16A559DA",
    "lastKnownWriteTime" : 1426193103,
    "path" : "~/GitHub/fuzzysummary/summ.R",
    "project_path" : "summ.R",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "source_on_save" : false,
    "type" : "r_source"
}