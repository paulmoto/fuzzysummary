#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize(text,5)
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-"New Orleans Saints' Will Smith plans to be back as long as the Saints want him"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
summary
features
title<-"Florida to Sue Georgia in U.S. Supreme Court Over Water - Bloomberg Business"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
require(tm)
require(frbs)
require(SnowballC)
require(NLP)
require(stringr)
require(openNLP)
require(openNLPmodels.en)
summarize<-function(text,n){
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-"Florida to Sue Georgia in U.S. Supreme Court Over Water - Bloomberg Business"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize(text,5)
text<-"AlchemyAPI has raised $2 million to extend the capabilities of its deep learning technology that applies artificial intelligence to read and understand web pages, text documents, emails, tweets, and other forms of content. Access Venture Partners led the Series A round, which the company will use to ramp up its sales and marketing, make hires and launch new services. Founder and CEO Elliot Turner says its natural language processing technology is used in vertical markets such as financial services, which uses Alchemy’s technology to analyze text and search for signals from the data it can plug into its trading algorithms. It is offered as a service or in the form of an appliance that companies install in its data centers. Alchemy, which launched in 2009, processes 3 billion API calls per month. It is used in 36 countries. The company does so many API calls that Programmable Web added Alchemy to its API Billionaire’s Club, joining Google, Facebook and LinkedIn and the few others who can claim such status. Alchemy gets paid by the API call or transaction. About 90 percent of the calls are transaction based. Alchemy has technology similar to IBM Watson, which customers use for questions and answers of particular data sets. In contrast, Turner says Alchemy does text analysis across any data set. The company is one of a handful to use neural net technology in such a fashion. Google is using it to do brain simulations but not on the par of Alchemy, which offers the capability to do queries on broad sets of disparate data sets. For example, it can be used to do fine-grained analysis for purposes of legal discovery. The work Alchemy is doing represents a major breakthrough in how data is analyzed. But can it compete with the big guns of the business? I think so. IBM has not offered Watson as a SaaS, which Alchemy does. IBM is offering deep learning but primarily as an on-premise solution. Alchemy offers a service that any company can pay by the sip.
"
#weird issue, text getting cut, replacing a \n by hitting enter fixed this.
summarize("AlchemyAPI Raises $2 Million For Neural Net Analysis Tech, On Par With IBM Watson, Google | TechCrunch",text,5)
require(tm)
require(frbs)
require(SnowballC)
require(NLP)
require(stringr)
require(openNLP)
require(openNLPmodels.en)
summarize<-function(title,text,n){
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize("AlchemyAPI Raises $2 Million For Neural Net Analysis Tech, On Par With IBM Watson, Google | TechCrunch",text,5)
text<-"
WASHINGTON — Even in a Congress where bipartisanship and comity are now officially the exceptions to the regular order, the near implosion on Capitol Hill on Thursday was notable, as both chambers erupted in a furor that went on for much of the day. In the Senate, leaders fought bitterly over proposed changes to Senate rules that would limit the filibuster, with Senator Harry Reid of Nevada, the Democratic leader, trading barbs with Senator Mitch McConnell of Kentucky, the Republican leader, several times on the Senate floor. “These are dark days in the history of the Senate,” Mr. McConnell said ominously, adding that the rule change suggested by Mr. Reid would lead to the Democrat’s being remembered as “the worst leader of the Senate ever.” The two men, both crafty lawmakers and once seemingly friends, seem now barely able to countenance each other’s presence. Over in the House, Republicans — without the help of a single Democrat — passed perhaps the most partisan farm bill in recent history, stripping out the food stamp program to attract enough support from conservatives. Just passing the bill, which was prepared in the House Rules Committee in the dark of night on Wednesday, proved an unusual chore as furious Democrats pulled one procedural move after another to delay the inevitable, taking turns to disparage the bill from the floor. “I am not proud of what you’re seeing here today,” said Representative Tim Walz, Democrat of Minnesota. “The disrespect shown to this hallowed ground by hatching this abomination in the middle of the night and forcing it here because of this extremist element is the reason that the American people think higher of North Korea than they do of this body.” The chaos reflects the reality that Congress has largely been reduced from a lawmaking entity to a political operation, in which positions are taken and fermented largely in the name of maintaining party unity rather than attracting votes from the other side. In the House, under the rule of Republicans, the minority is largely powerless to do anything but protest. Senate Republicans at least have the power to filibuster, which helps explain why they are so adamantly opposed to the Democrats’ gambit. Some Senate Republicans, after years of asking the majority to pass a budget, now refuse to allow the House and Senate to reconcile their two versions, less because of policy disagreements and more, it would appear, to appeal to a base that simply abhors deal-making. For their part, Democrats had been enjoying a stalemate over the issue of student loans because of its potential for campaign advertising copy next year. Bills come together now more often because of a breakdown in party unity, as was the case this week with the student loan bill, when Democrats showed signs of yielding because of disagreements within their own party over how to proceed. Likewise, Republicans occasionally will adopt Senate solutions only when their party is divided over their own bills, as was the case this year when they were forced to swallow a fiscal deal that included tax increases on high earners. The exception to the misery remains, at least largely, in the Appropriations Committees, owing perhaps to its now-waning tradition of benevolence as lawmakers take care of one another’s needs. Even as bills passed through the Senate committee on Thursday largely along party lines, laughter filled the hearing room as members joked with the amiable chairwoman of the committee, Senator Barbara A. Mikulski, Democrat of Maryland. “Illinois is now so tooted,” she noted as Senator Mark S. Kirk, a Republican, asked to toot his state’s horn in recognition of its role in creating an AIDS drug. The divide in Congress stems largely from deep disagreements “over the role and scope of government,” said Senator Angus King, independent of Maine, prompted by the 2010 Republican wave that filled the House with lawmakers devoted to reversing years of deficit spending brought on by tax cuts and increased entitlement spending. But the reality of sequestration, which set significant spending caps, combined with the ban on earmarks — once used to grease the wheels of legislation — have combined to make even historically bipartisan measures like the farm bill fraught with discord. “Sequestration is a meat cleaver,” Senator Bill Nelson, Democrat of Florida, said Thursday on the Senate floor. Money, at the end of the day — when available — remains the great unifier. To wit: a bipartisan immigration bill was possible in the Senate in large part because of billions of dollars in border-control spending, both for agents and equipment, that was added to it to appease conservatives, as well as a jobs program that secured support from some of the chamber’s most liberal members. A measure to provide relief to victims of Hurricane Sandy was similarly loaded with aid to states that suffered previous disasters. Even the Appropriations Committee, while still the place of civility, shows the strains of the pinch. In the Senate committee Thursday, members lamented an amendment to an appropriations bill that would have given more money to a program that would supplement heating and cooling costs for low-income people, but that, in this new era of tight purse strings, would pay for it by cutting into education grant money, as a “Hobson’s choice.” Senator Mary L. Landrieu, Democrat of Louisiana, angrily criticized the choice, which she said made her stomach hurt, setting off one of the few unpleasant exchanges of the day among a group of senators. “The essence of what we are now discussing is the choice between helping poor families to stay warm or to send our kids to college,” she said. “It’s sad.” The measure passed, but the debate over whether paying for one group’s needs at the expense of another’s is, Ms. Mikulski warned, “a much larger issue that can’t be resolved by the debate here today. This is really kind of where we are.”
"
#weird issue, text getting cut, replacing a \n by hitting enter fixed this.
text
text<-"
WASHINGTON — Even in a Congress where bipartisanship and comity are now officially the exceptions to the regular order, the near implosion on Capitol Hill on Thursday was notable, as both chambers erupted in a furor that went on for much of the day. In the Senate, leaders fought bitterly over proposed changes to Senate rules that would limit the filibuster, with Senator Harry Reid of Nevada, the Democratic leader, trading barbs with Senator Mitch McConnell of Kentucky, the Republican leader, several times on the Senate floor. “These are dark days in the history of the Senate,” Mr. McConnell said ominously, adding that the rule change suggested by Mr. Reid would lead to the Democrat’s being remembered as “the worst leader of the Senate ever.” The two men, both crafty lawmakers and once seemingly friends, seem now barely able to countenance each other’s presence. Over in the House, Republicans — without the help of a single Democrat — passed perhaps the most partisan farm bill in recent history, stripping out the food stamp program to attract enough support from conservatives. Just passing the bill, which was prepared in the House Rules Committee in the dark of night on Wednesday, proved an unusual chore as furious Democrats pulled one procedural move after another to delay the inevitable, taking turns to disparage the bill from the floor. “I am not proud of what you’re seeing here today,” said Representative Tim Walz, Democrat of Minnesota. “The disrespect shown to this hallowed ground by hatching this abomination in the middle of the night and forcing it here because of this extremist element is the reason that the American people think higher of North Korea than they do of this body.” The chaos reflects the reality that Congress has largely been reduced from a lawmaking entity to a political operation, in which positions are taken and fermented largely in the name of maintaining party unity rather than attracting votes from the other side. In the House, under the rule of Republicans, the minority is largely powerless to do anything but protest. Senate Republicans at least have the power to filibuster, which helps explain why they are so adamantly opposed to the Democrats’ gambit. Some Senate Republicans, after years of asking the majority to pass a budget, now refuse to allow the House and Senate to reconcile their two versions, less because of policy disagreements and more, it would appear, to appeal to a base that simply abhors deal-making. For their part, Democrats had been enjoying a stalemate over the issue of student loans because of its potential for campaign advertising copy next year. Bills come together now more often because of a breakdown in party unity, as was the case this week with the student loan bill, when Democrats showed signs of yielding because of disagreements within their own party over how to proceed. Likewise, Republicans occasionally will adopt Senate solutions only when their party is divided over their own bills, as was the case this year when they were forced to swallow a fiscal deal that included tax increases on high earners. The exception to the misery remains, at least largely, in the Appropriations Committees, owing perhaps to its now-waning tradition of benevolence as lawmakers take care of one another’s needs. Even as bills passed through the Senate committee on Thursday largely along party lines, laughter filled the hearing room as members joked with the amiable chairwoman of the committee, Senator Barbara A. Mikulski, Democrat of Maryland. “Illinois is now so tooted,” she noted as Senator Mark S. Kirk, a Republican, asked to toot his state’s horn in recognition of its role in creating an AIDS drug. The divide in Congress stems largely from deep disagreements “over the role and scope of government,” said Senator Angus King, independent of Maine, prompted by the 2010 Republican wave that filled the House with lawmakers devoted to reversing years of deficit spending brought on by tax cuts and increased entitlement spending. But the reality of sequestration, which set significant spending caps, combined with the ban on earmarks — once used to grease the wheels of legislation — have combined to make even historically bipartisan measures like the farm bill fraught with discord. “Sequestration is a meat cleaver,” Senator Bill Nelson, Democrat of Florida, said Thursday on the Senate floor.
Money, at the end of the day — when available — remains the great unifier. To wit: a bipartisan immigration bill was possible in the Senate in large part because of billions of dollars in border-control spending, both for agents and equipment, that was added to it to appease conservatives, as well as a jobs program that secured support from some of the chamber’s most liberal members. A measure to provide relief to victims of Hurricane Sandy was similarly loaded with aid to states that suffered previous disasters. Even the Appropriations Committee, while still the place of civility, shows the strains of the pinch. In the Senate committee Thursday, members lamented an amendment to an appropriations bill that would have given more money to a program that would supplement heating and cooling costs for low-income people, but that, in this new era of tight purse strings, would pay for it by cutting into education grant money, as a “Hobson’s choice.” Senator Mary L. Landrieu, Democrat of Louisiana, angrily criticized the choice, which she said made her stomach hurt, setting off one of the few unpleasant exchanges of the day among a group of senators. “The essence of what we are now discussing is the choice between helping poor families to stay warm or to send our kids to college,” she said. “It’s sad.” The measure passed, but the debate over whether paying for one group’s needs at the expense of another’s is, Ms. Mikulski warned, “a much larger issue that can’t be resolved by the debate here today. This is really kind of where we are.”
"
title<-"
A Day of Friction Notable Even for a Fractious Congress - NYTimes.com
"
title
title<-"A Day of Friction Notable Even for a Fractious Congress - NYTimes.com"
summarize(title,text,5)
text<-"
A refreshing burst of Canadian coolness is now sweeping into our area, eradicating the heat and humidity of the past few days. We’re on track for at least two spectacular days today and tomorrow and possibly more to follow. The weekend is more of a mixed bag with Saturday having the better chance to stay dry and Sunday showing an increasing risk of showers, but still a good bit of uncertainty at this point. What is almost certain, however, is that we have no 90s anticipated through early next week! Today (Wednesday): The first of a trio of tremendous weather days as Canadian cooling comes in full force to chase the dog days of summer away.  High pressure builds in and flushes the area with low humidity (dew points could dip into the 40s this afternoon), holding temperatures below normal with highs only in the upper 70s to near 80. Mostly sunny skies complete this pretty picture as breezes flow from the north and northwest near 15 mph with gusts near 25 mph.  Confidence: High Tonight:  Mostly clear and cool with very low humidity as temperatures plummet into the 50s in most areas.  Parts of the city should hold in the very comfortable upper 50s for lows, while the outer suburbs aim for quite cool conditions in the low 50s (the record is unreachable down in the 40s out at Dulles). Light breezes from the north. Confidence: High For related traffic news, check out Dr. Gridlock. Keep reading for the forecast through the weekend… Tomorrow (Thursday):  Sequels are rarely as good as the original, but in this potential trilogy of Nice Days, I believe Thursday will be just as great. Skies look to be sunny with low humidity, light winds, and temperatures delightful once again with highs in the upper 70s to low 80s. Confidence: High Tomorrow night:  Another very comfortable evening is heading our way with lows ranging from the middle 50s to low 60s as the skies stay mostly clear and the winds light. Confidence: High Friday should be another fantastic day for our triple play of Canadian cooling.  There is a chance that clouds increase some during the afternoon, but for now it looks like they’ll hold off late enough to qualify as a mostly sunny day. Highs are again only in the upper 70s to low 80s.  Friday night sees partly cloudy skies with slightly warmer lows in the low-to-mid 60s. Still not bad for mid-August! Confidence: Medium The weekend forecast is a bit more complicated. Saturday is probably the better day with highs in the low-to-mid 80s, partly cloudy skies, and a 30% chance of showers mainly south of D.C. Saturday night looks to be partly to mostly cloudy with a 30% chance of showers area-wide and lows in the 60s. Sunday continues partly to mostly cloudy with highs in the upper 70s to mid-80s and a 50% chance of showers as a disturbance tries to move closer from the south. There’s still a chance the system stays too far south, keeping the weekend mostly dry. Confidence: Low
"
title<-"D.C. area forecast: Ultra-nice weather dominates next few days - The Washington Post"
#weird issue, text getting cut, replacing a \n by hitting enter fixed this.
summarize(title,text,5)
text<-"
Aug. 14 (Bloomberg) -- Florida plans to file a U.S. Supreme Court lawsuit against Georgia, saying the state is consuming too much water that would otherwise flow to Florida, the latest battle nationally over an increasingly scarce resource. The dispute is fueled by the rapid growth of the metropolitan area surrounding Atlanta, which is demanding more water and hurting the oyster industry in Northwest Florida, Florida Governor Rick Scott, 60, said yesterday. Scott, a Republican, said he would file suit next month after the two states couldn’t reach an agreement. “That’s our water,” Scott told reporters while standing next to the Apalachicola Bay in the Florida Panhandle. “They’ve impacted our families. They’ve impacted the livelihood of people down here.” For more than 20 years, Florida, Georgia and Alabama have been mired in negotiations over the distribution of water shared by the three states. The dispute is emblematic of an increasingly common challenge facing cities and states across the country: Demand for water is outpacing supply as urban development and population growth sap resources. Urban development in Georgia has led to an increased need for water, much of it pumped from a river basin that’s also relied on by Florida and Alabama. ‘Unchecked Consumption’ Georgia has engaged in “unchecked consumption of water,” while not negotiating in good faith, making a lawsuit the only way to resolve the matter, Scott said in a statement. Georgia Governor Nathan Deal, 70, a Republican, said in a statement that Scott’s planned lawsuit is a “frivolous waste of time and money.” “Scott’s threat to sue my state in the U.S. Supreme Court greatly disappoints me after I negotiated in good faith for two years,” Deal said. “More than a year ago, I offered a framework for a comprehensive agreement. Florida never responded.” Legal disputes between states must be heard by the U.S. Supreme Court, instead of going through lower courts first, according to the Constitution. The U.S. Army Corps of Engineers is responsible for managing the water in the states’ shared river basin, which spans the Apalachicola, Chattahoochee and Flint rivers. Officials in Alabama, which has also fought Georgia over water distribution, haven’t said whether they’ll join in the lawsuit. The state will consider “all available options” to protect its water rights, said Jennifer Ardis, a spokeswoman for Alabama Governor Robert Bentley, a Republican, in an e-mail. At a hearing yesterday in Apalachicola, U.S. Senator Bill Nelson, a Florida Democrat, and U.S Senator Marco Rubio, a Florida Republican, both blamed Georgia for taking more than its fair share of water. Georgia’s consumption, along with a drought last year, threatens fisheries and economic development in the Florida Panhandle, they said. The oyster industry in Apalachicola Bay has collapsed over the past year. The National Oceanic and Atmospheric Administration issued a fishery disaster declaration on Aug. 12 over oysters. Scott and Nelson have both pushed for the disaster declaration during the past year. “The changes to water flow have decimated a once booming industry, but I’m hopeful we can soon start to turn things around,” Nelson said in a statement. Apalachicola Bay supplies 10 percent of the nation’s oysters, according to a December report by Florida’s Fish and Wildlife Conservation Commission. The industry supports about 2,500 jobs in Florida, according to the report. Many of those jobs, and perhaps the industry, are at risk due to the lack of fresh water flowing into the bay, Rubio said. “We don’t have time,” said Rubio. “In a couple years, there may not be anybody left to save around here in this industry.” To contact the reporters on this story: Toluse Olorunnipa in Tallahassee, Florida at tolorunnipa@bloomberg.net; Michael C. Bender in Washington at mbender10@bloomberg.net To contact the editor responsible for this story: Stephen Merelman at smerelman@bloomberg.net
"
title<-"Florida to Sue Georgia in U.S. Supreme Court Over Water - Bloomberg Business"
#weird issue, text getting cut, replacing a \n by hitting enter fixed this.
summarize(title,text,5)
