sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
text<-"New Orleans Saints outside linebacker Will Smith doesn't plan to give up football anytime soon. And even a major knee injury hasn't changed that.\nSmith tore his ACL on Aug. 25 during a preseason game against the Houston Texans and was placed on injured reserve. He's been spending most of his time in Pensacola, Fla., rehabbing.\u00A0\nSmith, 32, spoke to the media while in New Orleans on Tuesday for his celebrity waiter luncheon. The charity event was put on by his \"Where There's a Will There's a Way\" foundation, which benefits the Bridge House/Grace House of New Orleans.\n\"We've all been touched by family members that had substance abuse problems ... and it's something that we're very passionate about,\" Smith said. \"We want to continue to do as much as we can for the cause.\"\nIt's been an unusual time of year for the 10-year veteran, who rarely missed games during his first nine seasons in New Orleans. Instead of making tackles on the field, he's been chased around his home his three kids.\u00A0\nIt's all good as long as they avoid his injured leg, he said with a smile.\nBut the time away hasn't ebbed his passion for the game. Smith said his only timetable for retirement is when playing ceases to be fun.\u00A0\n\"First step is, I've got to get healthy, running around doing the things I do, and after that I'll be back,\" he said. \"I'll be back as long as the Saints want me back. ... I want to play until I don't want to play anymore, until it becomes something not desirable for me.\"\nIt was a difficult pill for Smith swallow when he learned he would be placed on injured reserve. He'd never missed more than a few games at a time before.\n\"I didn't really know how to take it in,\" Smith said. \"But life is a struggle, you have good struggles and bad struggles and this is one of the bad struggles. I figure I have to work through it and do my best and good things will come out of it.\"\nIn the meantime, he has been relegated to watching games on TV. And even he was on the edge of his seat during the Saints' loss to the New England Patriots--at least from what he saw of it.\n\"I was watching it at my home and we actually changed the channel because we thought the game was over,\" he said. \"I still haven't seen the ending. I got a bunch of texts from my friends that watched the game that said, 'What just happened?' I was like, what are you talking about?' ... That's football. You just never know.\"\nSmith said the play of the defense, which is giving up only 17.2 points-per-game, has been no surprise to him after he went through the offseason with the unit.\n\"Last year was last year. The guys are all motivated to be better than what we were last year,\" he said. \"And they came out and worked hard all offseason, all OTAs, camp. They wanted to be better and they went out and did it. It's something that from the outside looking in, I kind of assumed everyone thought we'd be better but they didn't expect us to be as good as we are now.\"\nSmith gave credit to defensive coordinator Rob Ryan for the enhanced play this season.\n\"It's totally different than what we did last year. It kind of goes back to some of the similar things we did with Gregg Williams, which kind of fits our personnel group,\" Smith said. \"Guys are motivated more and when those guys are motivated, the sky's the limit.\"\nBut despite how well they've been playing, he said he still wishes he could be out there helping.\n\"They've been doing phenomenal so far, we're 5-1 going into the bye, guys have been playing really, really well,\" he said. \"It looks like we're going to have a really good season. Even though I'm not out there, I'm still excited. I'm always going to be a Saints fan.\"\nSo has retirement even been a thought?\n\"I'm always thinking about that,\" he said. \"Now, I have more time to be with the family, to be with my kids, do a lot of other stuff ... that football has prevented me from doing. That doesn't change the fact that I love to play football and love to be out there flying around. If anything, it motivates me more.\"
Katherine Terrell can be reached at KTerrell@nola.com or 504.826.3405.\u00A0\n\u00A0NOLA.com Saints coverage on Facebook \n"
#weird issue, text getting cut, replacing a \n by hitting enter fixed this.
summarize(text,3)
require(tm)
require(frbs)
require(SnowballC)
require(NLP)
require(stringr)
require(openNLP)
require(openNLPmodels.en)
require(lsa)
require(igraph)
summarize<-function(text,n){
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-"New Orleans Saints' Will Smith plans to be back as long as the Saints want him"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,5,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize(text,3)
require(tm)
require(frbs)
require(SnowballC)
require(NLP)
require(stringr)
require(openNLP)
require(openNLPmodels.en)
require(lsa)
require(igraph)
summarize<-function(text,n){
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-"New Orleans Saints' Will Smith plans to be back as long as the Saints want him"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize(text,3)
text<-"New Orleans Saints outside linebacker Will Smith doesn't plan to give up football anytime soon. And even a major knee injury hasn't changed that.\nSmith tore his ACL on Aug. 25 during a preseason game against the Houston Texans and was placed on injured reserve. He's been spending most of his time in Pensacola, Fla., rehabbing.\u00A0\nSmith, 32, spoke to the media while in New Orleans on Tuesday for his celebrity waiter luncheon. The charity event was put on by his \"Where There's a Will There's a Way\" foundation, which benefits the Bridge House/Grace House of New Orleans.\n\"We've all been touched by family members that had substance abuse problems ... and it's something that we're very passionate about,\" Smith said. \"We want to continue to do as much as we can for the cause.\"\nIt's been an unusual time of year for the 10-year veteran, who rarely missed games during his first nine seasons in New Orleans. Instead of making tackles on the field, he's been chased around his home his three kids.\u00A0\nIt's all good as long as they avoid his injured leg, he said with a smile.\nBut the time away hasn't ebbed his passion for the game. Smith said his only timetable for retirement is when playing ceases to be fun.\u00A0\n\"First step is, I've got to get healthy, running around doing the things I do, and after that I'll be back,\" he said. \"I'll be back as long as the Saints want me back. ... I want to play until I don't want to play anymore, until it becomes something not desirable for me.\"\nIt was a difficult pill for Smith swallow when he learned he would be placed on injured reserve. He'd never missed more than a few games at a time before.\n\"I didn't really know how to take it in,\" Smith said. \"But life is a struggle, you have good struggles and bad struggles and this is one of the bad struggles. I figure I have to work through it and do my best and good things will come out of it.\"\nIn the meantime, he has been relegated to watching games on TV. And even he was on the edge of his seat during the Saints' loss to the New England Patriots--at least from what he saw of it.\n\"I was watching it at my home and we actually changed the channel because we thought the game was over,\" he said. \"I still haven't seen the ending. I got a bunch of texts from my friends that watched the game that said, 'What just happened?' I was like, what are you talking about?' ... That's football. You just never know.\"\nSmith said the play of the defense, which is giving up only 17.2 points-per-game, has been no surprise to him after he went through the offseason with the unit.\n\"Last year was last year. The guys are all motivated to be better than what we were last year,\" he said. \"And they came out and worked hard all offseason, all OTAs, camp. They wanted to be better and they went out and did it. It's something that from the outside looking in, I kind of assumed everyone thought we'd be better but they didn't expect us to be as good as we are now.\"\nSmith gave credit to defensive coordinator Rob Ryan for the enhanced play this season.\n\"It's totally different than what we did last year. It kind of goes back to some of the similar things we did with Gregg Williams, which kind of fits our personnel group,\" Smith said. \"Guys are motivated more and when those guys are motivated, the sky's the limit.\"\nBut despite how well they've been playing, he said he still wishes he could be out there helping.\n\"They've been doing phenomenal so far, we're 5-1 going into the bye, guys have been playing really, really well,\" he said. \"It looks like we're going to have a really good season. Even though I'm not out there, I'm still excited. I'm always going to be a Saints fan.\"\nSo has retirement even been a thought?\n\"I'm always thinking about that,\" he said. \"Now, I have more time to be with the family, to be with my kids, do a lot of other stuff ... that football has prevented me from doing. That doesn't change the fact that I love to play football and love to be out there flying around. If anything, it motivates me more.\"\nKatherine Terrell can be reached at KTerrell@nola.com or 504.826.3405.\u00A0\n\u00A0NOLA.com Saints coverage on Facebook \n"
"
features
origsent[9]
summary
summarize(text,5)
summarize(text,3)
require(tm)
require(frbs)
require(SnowballC)
require(NLP)
require(stringr)
require(openNLP)
require(openNLPmodels.en)
require(lsa)
require(igraph)
summarize<-function(text,n){
text<-gsub("\n","\n\n",text)
paranot <-
Annotator(function(s, a = Annotation()) {
spans <- blankline_tokenizer(s)
n <- length(spans)
## Need n consecutive ids, starting with the next "free"
## one:
from <- next_id(a$id)
Annotation(seq(from = from, length.out = n),
rep.int("paragraph", n),
spans$start,
spans$end)
},
"A paragraph token annotator based on blankline_tokenizer().")
#text<-gsub("\n"," ",text)
#corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
wanot <- Maxent_Word_Token_Annotator()
posanot <- Maxent_POS_Tag_Annotator()
ann <- annotate(text,
list(anot,
wanot,
posanot,paranot))
anntext<-AnnotatedPlainTextDocument(text,ann)
sent_ann<-annotate(text,anot)
paralength<-vector()
for(i in 1:length(paras(anntext))){
paralength[i]<-length(paras(anntext)[[i]])
}
parapos<-vector()
for(i in 1:length(paralength)){
for(j in 1:paralength[i]){
parapos<-c(parapos,paralength[i]-(j-1))
}
}
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-"New Orleans Saints' Will Smith plans to be back as long as the Saints want him"
twords<-stripWhitespace(title)
twords<-tolower(unlist(strsplit(title," ")))
twords<-twords[!twords %in% stopwords('english')]
twords<-stemDocument(twords)
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
wordoccur<-vector() #sentences containing each word
wordscore<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
wordoccur[i]<-sum(dict[i,]>0)
wordscore[i]<-wordfreq[i]*log((length(origsent))/wordoccur[i])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
#sim<-matrix(NA,length(corpus),length(corpus))
#maxes<-vector()
#token matching
#for(i in 1:nrow(sim)){
#  for(j in 1:ncol(sim)){
#    sim[i,j]<-sum(pmin(as.vector(dict[,i]),as.vector(dict[,j])))
#  }}
#diag(sim)<-0
#for(i in 1:nrow(sim)){
#  if(max(sim[,i])>0){
#  sim[,i]<-sim[,i]/max(sim[,i])
#  }
#}
sentencevectors<-matrix(NA,nrow(dict),length(origsent))
for(i in 1:ncol(sentencevectors)){
sentencevectors[,i]<-as.vector(dict[,i]*wordscore)
}
sim<-matrix(NA,length(origsent),length(origsent))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
if(!i==j){
sim[i,j]<- sum(sentencevectors[,i]*sentencevectors[,j])/(sqrt(sum(sentencevectors[,i]^2))*sqrt(sum(sentencevectors[,j]^2)))
}
}
}
diag(sim)<-0
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(twords,wordcount,y=sent_words[[i]])))/length(twords)} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
if(length(sent_words[[i]])>0){features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])}#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i]) #similarity
features[i,7]<-sum(dict[,i]*wordscore) #word frequency
if(length(sent_words[[i]])>0) {features[i,8]<-str_count(origsent[[i]],"[A-Z][a-z]")/length(sent_words[[i]])} #Proper nouns?
}
#normalizing features
features[1:length(parapos),3]<-parapos
features[length(parapos):nrow(features),3]<-0
features[,3]<-features[,3]/max(features[,3])
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,6]<-features[,6]/max(features[,6])
features[,7]<-features[,7]/max(features[,7])
featureweights<-c(1,1,1,1,1,1,1,1)
sent_scores<-vector()
for(i in 1:nrow(features)){
sent_scores[i]<-sum(features[i,]*featureweights)
}
summary<-data.frame(origsent,sent_scores)
summary<-summary[order(-sent_scores),]
return(cat(as.character(summary[1:n,1][order(row.names(summary))][!is.na(summary[1:n,1][order(row.names(summary))])])))
}
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
summarize(text,5)
summarize(text,5)
