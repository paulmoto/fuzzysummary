nrow(dict)
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq
wordfreq[order(-wordfreq)]
row.names(wordfreq[order(-wordfreq)])
names(wordfreq[order(-wordfreq)])
row.names(wordfreq[order(-wordfreq)])
dict[[5]]
dict[[6]]
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
wordfreq
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
wordfreq
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)[order(-wordfreq),]
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)[order(-wordfreq),]
wordfreq
head(wordfreq)
wordfreq[1:5,1]
require(lsa)
install.packages(lsa)
install.packages("lsa")
require(lsa)
associate(dict)
associate(dict,"data")
dict
cosine(data)
cosine(inspect(data))
inspect(data)
cosine(dict)
cosine(inspect(dict))
inspect(dict)
cosine(inspect(dict))
las(dict)
lsa(dict)
is(lsa(dict))
(lsa(dict)$dk)
SVD<-lsa(dict)
cosine(SVD$dk)
LSAspace
SVD
cosine(SVD$dk)
inspect(dict)
wordfreq
SVD<-lsa(dict)$dk
cosine(SVD)
SVD<-lsa(dict)$dk
cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
for(i in 1:nrow(cosadj)){
for(j in 1:ncol(cosadj)){
if (!i==j){cosadj[i,j]<-1/acos((SVD[,i]%*%SVD[,j])/((sqrt(sum(SVD[,i]^2)))*(sqrt(sum(SVD[,j]^2)))))}
}
}
diag(cosadj)<-0
SVD<-lsa(dict)$dk
cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
for(i in 1:nrow(cosadj)){
for(j in 1:ncol(cosadj)){
if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
}
}
diag(cosadj)<-0
cosadj
g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
pgr<-page.rank(g)$vector
require(igraph)
g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
pgr<-page.rank(g)$vector
pgr
origsent
summ<-data.frame(origsent,pgr)[order(-pgr),]
summ
sim<-matrix(NA,length(corpus,length(corpus)))
sim<-matrix(NA,length(corpus),length(corpus))
a<-c(1,1,2)
b<-c(2,1,1)
min(a,b)
dict
nrow(dict)
sim<-matrix(NA,length(corpus),length(corpus))
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
int<-vector()
uni<-vector()
for (n in 1:nrow(dict)) {
int[n]<- min(dict[n,i],dict[n,j])
}
sim[i,j]<- sum(int)/max(int)
}}
sim<-matrix(NA,length(corpus),length(corpus))
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
int<-vector()
uni<-vector()
for (n in 1:nrow(dict)) {
int[n]<- min(dict[n,i],dict[n,j])
}
sim[i,j]<- sum(int)
}}
sim<-matrix(NA,length(corpus),length(corpus))
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
int<-vector()
uni<-vector()
for (n in 1:nrow(dict)) {
int[n]<- min(dict[n,i],dict[n,j])
}
sim[i,j]<- sum(int)/max(int)
}}
diag(sim)<-0
sum(sim[i,])
sum(sim[,i])
sim
min(0,0)
min(NaN,0)
max(NaN,0)
sim[sim==NaN]<-0
sim
sim[sim==NaN]
sim
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
int<-vector()
for (n in 1:nrow(dict)) {
int[n]<- min(dict[n,i],dict[n,j])
}
if(sum(int)>0){sim[i,j]<- sum(int)/max(int)}
else(sim[i,j]<-0)
}}
sim<-matrix(NA,length(corpus),length(corpus))
maxes<-vector()
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
int<-vector()
for (n in 1:nrow(dict)) {
int[n]<- min(dict[n,i],dict[n,j])
}
maxes[i]<-max(int)
if(sum(int)>0){sim[i,j]<- sum(int)}
else(sim[i,j]<-0)
}}
?pmin
pmin(dict[,1],dict[,2])
dict[,1]
dict[[1]]
dict[[2]]
inspect(dict)
pmin(inspect(dict)[,1],inspect(dict)[,2])
sum(pmin(inspect(dict)[,1],inspect(dict)[,2]))
sum(pmin(inspect(dict)[,1],inspect(dict)[,2]))
dict
dict[,1]
dict[,2]
sum(pmin(inspect(dict)[,1],inspect(dict)[,2]))
sum(pmin(inspect(dict)[,3],inspect(dict)[,2]))
sum(pmin(inspect(dict)[,7],inspect(dict)[,2]))
sum(pmin(inspect(dict)[,9],inspect(dict)[,2]))
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
sim[i,j]<-sum(pmin(inspect(dict)[,i],inspect(dict)[,j]))
}}
sum(pmin(inspect(dict)[,9],inspect(dict)[,2]))
sum(pmin(inspect(dict)[,9],inspect(dict)[,2],quiet=T))
dict
dict
image<-inspect(dict)
image
sim<-matrix(NA,length(corpus),length(corpus))
maxes<-vector()
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
sim[i,j]<-sum(pmin(image[,i],image[,j]))
}}
diag(sim)<-0
sim
for(i in 1:nrow(sim)){
sim[,i]<-sim[,i]/max(sim[,i])
}
sim
sim[sim==NaN]
sim<-matrix(NA,length(corpus),length(corpus))
maxes<-vector()
#token matching
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
sim[i,j]<-sum(pmin(image[,i],image[,j]))
}}
diag(sim)<-0
for(i in 1:nrow(sim)){
if(max(sim[,i])>0){
sim[,i]<-sim[,i]/max(sim[,i])
}
}
wordframe
wordfreq
thematic<-wordfreq[order(-wordfreq)[1:5],1]
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
thematic
wordfreq
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
thematic
wordfreq
image<-inspect(dict)
dict[,1]
image
image[,1]
image[,1]*wordfreq[,2]
sum(image[,1]*wordfreq[,2])
paranot<-Maxent_Chunk_Annotator(language = "en", probs = FALSE, model = NULL)
nchar("hello")
capcount<-function(string){
return(nchar(gsub('![^A-Z]+!','',string)))
}
capcount("LlaksdfAKAK")
gsub("![^A-Z]+!","",LlaksdfAKAK")
gsub("![^A-Z]+!","","LlaksdfAKAK")
gsub("![^A-Z]+!","","LlaksdfAKAK")
gsub("![A-Z]+!","","LlaksdfAKAK")
install.packages("C:/Users/admin/Downloads/openNLPmodels.en_1.5-1.tar.gz", repos = NULL, type = "source")
paranot<-Maxent_Chunk_Annotator(language = "en", probs = FALSE, model = NULL)
para_ann<-annotate(text,paranot)
para_ann<-annotate(text,paranot,sent_ann)
para_ann<-annotate(text,paranot,sent_ann,word_ann)
?annotate
paranot<-Maxent_Chunk_Annotator(language = "en", probs = FALSE, model = NULL)
para_ann<-annotate(text,paranot,sent_ann,word_ann)
para_ann<-annotate(text,paranot,word_ann)
para_ann<-annotate(text,paranot)
?Maxent_Chunk_Annotator
annotation<-annotate(text,list(anot,wanot,paranot,posanot))
posanot<-Maxent_POS_Tag_Annotator()
annotation<-annotate(text,list(anot,wanot,paranot,posanot))
annotation
annotation<-annotate(text,list(anot,wanot,paranot))
annotation<-annotate(text,list(anot,wanot,posanot))
annotation
annotate[,1]
is(annotate)
annotation[,1]
is(annotation)
text[annotation]
annotation
text[annotation[1]]
annotation[1]
annotation[1][1]
annotation[[1]]
annotation
annotation()
text<-"Data science is all the rage. Almost every CMO I know wants a data scientist for their very own – they are the status symbol du jour for senior executives everywhere. But, building the right data science team for your organization is not as easy as picking the right data scientist. Data science starts by asking the right questions, and the first question to ask is: What is data science?
Some people believe that data science is just a sexy name that mathletes made up to get better-paying jobs. For the sake of this writing, let’s define data science as “the analysis of data using the scientific method with the primary goal of turning information into action.”
How do they do it? Data scientists use a variety of mathematical tools to help answer questions and uncover patterns that contribute to the results, but it’s not just math. It's much, much more.
Data Science Venn Diagram
Foundational Skills
In order to turn information into action, you need a team that is proficient in the three foundational skills:
Domain Expertise – to define the problem space
Mathematics – for theoretical structure and problem
solving
Computer Science – to provide the environment where data is manipulated
Data science exists at the intersection of these three foundational skills; discounting or overweighting any of them will yield suboptimal results.
Domain Expertise
You know your business. In order to put data science to work, you are going to use 100 percent of your business knowledge, institutional memory and intuition to ask the right questions. Everyone wants to know how to increase sales – that’s question one. But domain experts can ask more specific questions that will yield measurable, actionable improvements, such as the following:
Can we improve productivity in XYZ Department by increasing the usability of ABC data sets?
Can increased access to scanner data, share of basket data, heuristic weather pattern data and parking lot density data increase our return on assets?
Can we use our product attributes data sets to improve competitiveness?
The more specific your questions are, the more likely you are to get actionable results.
Mathematics
There is a lot of math in data science. The mathematicians on your data science team will be world-class problem solvers. They will be experts in statistical modeling, signal processing, probability models, pattern recognition, predictive analytics and a bunch of subspecialties that you learned in college mathematics class but have long since forgotten.
Data science becomes magical when brilliant mathematical constructs are applied to big data sets (vast amounts of data too big for humans to deal with), yielding unexpected actionable insights. The best teams develop AI, pattern-matching and machine-learning tools that generate the building blocks for predictive models. Great mathematicians are a key component to any data science department, but they cannot and do not work alone.
Computer Science
Data science happens inside computer systems. It cannot exist anywhere else. Having the right architecture for your data science function is as important as having the right architecture for your physical work environment.
Is your current CTO/CIO knowledgeable about the technical requirements for your data science team? Big data requires special storage, special handling and special network capabilities. The tools are different, computer “horsepower” requirements are different – in fact, almost everything you need for your data science team will need to be purpose built, rented, borrowed or partnered with.
Data Science Readiness Assessments
How should you think about getting ready for data science? There is a short list of steps you should consider:
Formulate Questions: Get your key stakeholders together and formulate context-relevant questions and hypotheses to drive data scientific research. This is an important first step. It will set the stage for success.
Audit Data Assets: Assign a team or hire a consulting firm to audit your existing data sets and data-gathering systems. This will help with the creation of appropriate RFPs for potential partners, suppliers and potential acquisition targets.
Craft a Roadmap: Build a roadmap to get from where you are to having a working data science department by quantifying the best methods for identifying, obtaining and transforming data sets to make them suitable for the production of statistical evidence.
The Time Is Now!
Best-in-class companies realize the importance of analytics. The goal is a data-driven business strategy with an operating model that enables cross-functional collaboration, governance, metrics and change management. You’ll have to create methodologies to empower ongoing data scientific research. You will need to build or buy appropriate infrastructure, including analytics platforms, visualization tools and big data environments. You will find ways to manage data from 3rd-party partnerships, enforce data governance and develop best practices data munging and wrangling.
You will have the right resources:
Business Analysts for problem definition, solution design and analytics roadmaps
Research and Big Data Engineering for data science, experiment design and training
Model Development for data preparation, profiling and model building and validation
Operations for visualizations, QA, data management, maintenance and implementation
And then, you will be ready for data science.
Want Help?
We have a team ready to help you with your data science readiness assessment. Just shoot me an email, and I’ll be happy to work with you to help you achieve your business goals."
annotate(text,list(anot,wanot,paranot,posanot))
annotate(text,list(anot,wanot,posanot))
text<-gsub("\n"," ",text)
sentbound<-annotate(text,anot)
origsent<-text[sentbound]
sentanot
sentbound
text[sentbound]
sentce.boundoundaries<-annotate(text,anot)
sentence.boundoundaries<-annotate(text,anot)
text[sentence.boundoundaries]
require(tm)
require(SnowballC)
require(openNLPmodels.en)
require(lsa)
require(igraph)
text<-gsub("\n"," ",text)
corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
sent_ann<-annotate(text,anot)
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-c("big","data","team","analy")
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
sim<-matrix(NA,length(corpus),length(corpus))
maxes<-vector()
#token matching
image<-inspect(dict)
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
sim[i,j]<-sum(pmin(image[,i],image[,j]))
}}
diag(sim)<-0
for(i in 1:nrow(sim)){
if(max(sim[,i])>0){
sim[,i]<-sim[,i]/max(sim[,i])
}
}
capcount<-function(string){
return(nchar(gsub('![^A-Z]+!','',string)))
}
features<-matrix(NA,length(corpus),8)
for(i in 1:nrow(features)){
features[i,1]<- sum(unlist(lapply(title,wordcount,y=sent_words[[i]])))/length(sent_words[[i]]) #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- #sentence position
features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i])
features[i,7]<-sum(image[,1]*wordfreq[,2])
}
#normalizing features
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,7]<-features[,7]/max(features[,7])
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
require(tm)
require(SnowballC)
require(openNLPmodels.en)
require(lsa)
require(igraph)
text<-gsub("\n"," ",text)
corpus<-VCorpus(VectorSource(text))
anot<-Maxent_Sent_Token_Annotator(language = "en", probs = FALSE, model = NULL)
sent_ann<-annotate(text,anot)
origsent<-vector()
for(i in 1:length(sent_ann)){
origsent[i]<-substr(text,sent_ann$start[i],sent_ann$end[i])
}
corpus<-VCorpus(VectorSource(origsent))
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeWords,stopwords("english"))
corpus<-tm_map(corpus,stemDocument)
sent_words<-list()
for(i in 1:length(corpus)){
sent_words[[i]]<- MC_tokenizer(corpus[[i]])[!MC_tokenizer(corpus[[i]])==""]
}
dict<-TermDocumentMatrix(VCorpus(VectorSource(sent_words)), control = list())
#Title words Feature
title<-c("big","data","team","analy")
#wordcount counts number of times x appears in y
wordcount<- function(x,y){
return(length(grep(sprintf("^%s$",x),y)))
}
#frequency of words in document
wordfreq<-vector()
for(i in 1:nrow(dict)){
wordfreq[i]<-sum(dict[i,])
}
wordfreq<-data.frame(dict[[6]]$Terms,wordfreq)
thematic<-wordfreq[order(-wordfreq[,2])[1:5],1]
sim<-matrix(NA,length(corpus),length(corpus))
maxes<-vector()
#token matching
image<-inspect(dict)
for(i in 1:nrow(sim)){
for(j in 1:ncol(sim)){
sim[i,j]<-sum(pmin(image[,i],image[,j]))
}}
diag(sim)<-0
for(i in 1:nrow(sim)){
if(max(sim[,i])>0){
sim[,i]<-sim[,i]/max(sim[,i])
}
}
capcount<-function(string){
return(nchar(gsub('![^A-Z]+!','',string)))
}
features<-matrix(NA,length(corpus),8)
for(i in 1:nrow(features)){
features[i,1]<- sum(unlist(lapply(title,wordcount,y=sent_words[[i]])))/length(sent_words[[i]]) #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i])
features[i,7]<-sum(image[,1]*wordfreq[,2])
}
#normalizing features
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,7]<-features[,7]/max(features[,7])
#SVD<-lsa(dict)$dk
#
#cosadj<-matrix(NA,nrow(SVD),nrow(SVD))
#cosine distance = angle between vectors [0,pi/2]
#for(i in 1:nrow(cosadj)){
#  for(j in 1:ncol(cosadj)){
#    if (!i==j){cosadj[i,j]<-1/acos((SVD[i,]%*%SVD[j,])/((sqrt(sum(SVD[i,]^2)))*(sqrt(sum(SVD[j,]^2)))))}
#  }
#}
#diag(cosadj)<-0
#g<-graph.adjacency(cosadj,weighted=T,mode='undirected')
#pgr<-page.rank(g)$vector
#summ<-data.frame(origsent,pgr)[order(-pgr),]
#cosine distance = cosine(inspect(dict))
features
origsent[7]
sent_words[1]
sent_words[7]
features<-matrix(0,length(corpus),8)
for(i in 1:nrow(features)){
if(length(sent_words[[i]])>0){features[i,1]<- sum(unlist(lapply(title,wordcount,y=sent_words[[i]])))/length(sent_words[[i]])} #Title Words
features[i,2]<- length(sent_words[[i]]) #sentence length
features[i,3]<- NA#sentence position
features[i,4]<- sum(!is.na(as.numeric(strsplit(origsent[[i]]," ")[[1]])))/length(sent_words[[i]])#numerical data
features[i,5]<- sum(unlist(lapply(thematic,wordcount,y=sent_words[[i]])))#thematic words top 5 words
features[i,6]<- sum(sim[,i])
features[i,7]<-sum(image[,i]*wordfreq[,2])
}
#normalizing features
features[,2]<-features[,2]/max(features[,2])
features[,5]<-features[,5]/max(features[,5])
features[,7]<-features[,7]/max(features[,7])
warnings()
features
gsub("![A-Z]","","ASasDFdf")
gsub("[A-Z]","","ASasDFdf")
gsub("[a-z]","","ASasDFdf")
gsub("[^A-Z]","","ASasDFdf")
nchar(gsub("[^A-Z]","","ASasDFdf"))
nchar(gsub("[^A-Z]","",origsent[1]))
nchar(gsub("[^A-Z]","",origsent[[1]))
nchar(gsub("[^A-Z]","",origsent[[1]]))
nchar(gsub("[^A-Z]","",origsent[[2]]))
origsent[2]
(gsub("[^A-Z]","",origsent[[2]]))
